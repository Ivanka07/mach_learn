import tensorflow as tf
import numpy as np
import os
import datetime

#some code samples have been taken from https://habr.com/company/nixsolutions/blog/416129/
print('Starting up with tensorflow version = %s' % tf.__version__)

NUM_HIDDEN_LAYERS = 4
NUM_NEURONS = 10
EXP_DECAY = 0.5

#TODO: add minibatch normalization 

def create_placeholders(d_input, g_input, d_learning_rate, g_leanring_rate):
    '''
    Creates placeholders for models
    :param d_input: tuple containing dimensions for D 
    :param g_input: tuple containing dimensions for G
    '''
    assert d_input!=None
    assert g_input!=None
    d_input_pl = tf.placeholder(tf.float32, (None, *d_input), name='inputs_disc') 
    g_input_pl = tf.placeholder(tf.float32, (None, g_input), name='inputs_gen') 
    
    d_learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_disc') 
    g_learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_gen')
    return d_input_pl, g_input_pl, d_learning_rate_pl, g_learning_rate_pl

def create_losses(input_real, input_gen, output_channel_dim, alpha):
    '''
    Returns losses for Generator and Discrimanator
    :param input_real: samples from training dataset,
    :param input_gen: samples generated by generator,
    :param output_channel_dim: the number of channels in output sample
    :return: a tuple of (discriminator loss, generator loss)
    '''
    generated = generator(input_gen)
    d_real_tensor, d_logits_real = discriminator(input_real)
    d_gen_tensor, d_gen_logits = discriminator(generated, reuse_var=True)
    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_real_tensor)))
    d_loss_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_gen_logits, labels=tf.zeros_like(d_gen_tensor)))
    d_loss = d_loss_real + d_loss_gen
    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_gen_logits, labels=tf.ones_like(d_gen_tensor)))
    #d_logits are the labels
    return d_loss, g_loss


def create_optimizers(d_loss, g_loss, d_learn_rate, g_learn_rate, exp_decay):
    nn_vars = tf.trainable_variables()
    d_vars = [var for var in nn_vars if  var.name.startswith('discriminator')]
    g_vars = [var for var in nn_vars if  var.name.startswith('generator')]
    d_train_optimizer = tf.train.AdamOptimizer(learning_rate = d_learn_rate, beta1=exp_decay).minimize(d_loss, var_list=d_vars)
    g_train_optimizer = tf.train.AdamOptimizer(learning_rate = g_learn_rate, beta1=exp_decay).minimize(g_loss, var_list=g_vars)

    return d_train_optimizer, g_train_optimizer
    
    

def generator(input_vec, reuse_var=False):
    '''
    We build a generator network here,
    :param input_vec: input tensor with samples of latent vector z,
    :param reuse_var: reuse of the varibles in scope,
    :return: a generated sample
    '''
    assert input_vec !=None
    with tf.variable_scope('generator', reuse=reuse_var):
        print('Creating generator model') 
        initializer = tf.truncated_normal_initializer(stddev=0.02)
        hl = tf.layers.dense(input_vec, 20, activation=tf.nn.leaky_relu, kernel_initializer=initializer, name='hl1')
        for l in range(0, NUM_HIDDEN_LAYERS):
            _name = 'hl_' + str(l)
            l = tf.layers.dense(hl, 20, activation=tf.nn.leaky_relu, kernel_initializer=initializer, name=_name)
            hl = l
        logits  = tf.layers.dense(hl, units = 1, activation=None, name='logits')
        #out = tf.sigmoid(logits, name='logits')
        out = tf.sigmoid(logits, name='out')
        return out



def discriminator(input_vec, reuse_var=False):
    '''
    We build a discriminator network here,
    :param input_vec: input tensor with samples that represent lines,
    :param output_shape: shape of output tensor,
    :param reuse: reuse of the varibles in scope
    '''
    with tf.variable_scope('discriminator', reuse=reuse_var):
        print('Creating discriminator model')
        initializer = tf.truncated_normal_initializer(stddev=0.02)
        hl = tf.layers.dense(input_vec, NUM_NEURONS, activation=tf.nn.leaky_relu, kernel_initializer=initializer, name='hl')
        for l in range(0, NUM_HIDDEN_LAYERS):
            _name = 'hl_' + str(l)
            l = tf.layers.dense(hl, NUM_NEURONS, activation=tf.nn.leaky_relu, kernel_initializer=initializer, name=_name)
            hl = l
        logits  = tf.layers.dense(hl, units = 1, activation=None, name='logits')
        out = tf.sigmoid(logits)
        return out, logits



def train_gan(batch_size, epoch_num, z_dim, d_learning_rate, g_learning_rate, batches):
    #create input placeholders
    d_input_pl, g_input_pl, d_lr_pl, g_lr_pl = create_placeholders((20,1), z_dim, d_learning_rate, g_learning_rate) 
    #create losses
    d_loss, g_loss = create_losses(d_input_pl, g_input_pl, None, None)
    #create optimizer
    d_opt, g_opt = create_optimizers(d_loss, g_loss, d_learning_rate, g_learning_rate, EXP_DECAY) 
    tf.get_variable_scope().reuse_variables()    
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:
        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver()
        tf.summary.scalar('Generator loss', g_loss)
        tf.summary.scalar('Decriminator loss', d_loss)
        merged = tf.summary.merge_all() 
        
        if not os.path.exists('./tensorboard/'):
            os.makedirs('./tensorboard')
        logdir = 'tensorboard/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '/'
        writer = tf.summary.FileWriter(logdir, sess.graph)
        
        for epoch in range(0, epoch_num):
           for i in range (0, batches.shape[0]):
                batch = batches[i,:,:]
                #create vector with random noise
                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))
                # Run optimizers
                _ = sess.run(d_opt, feed_dict={d_input_pl: batch, g_input_pl: batch_z, d_lr_pl: d_learning_rate})
                _ = sess.run(g_opt, feed_dict={d_input_pl: batch, g_input_pl: batch_z, g_lr_pl: g_learning_rate})
                if i % 10 == 0:
                    d_train_loss = d_loss.eval({g_input_pl: batch_z, d_input_pl: batch})
                    g_train_loss = g_loss.eval({g_input_pl: batch_z})
                    tf.summary.scalar('Training generator loss', d_train_loss)
                    tf.summary.scalar('Training discriminator loss', d_train_loss)
                    z_batch = np.random.normal(-1, 1, size=[batch_size, z_dim])
                    summary = sess.run(merged, {g_input_pl: z_batch, d_input_pl: batch})
                    writer.add_summary(summary,i)
           
        saver.save(sess, "./models/line_gan_model.ckpt")
